# -*- coding: utf-8 -*-
"""text_summerization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cpnz3UXu0-I4QNzk80m2ubOYbrN8Bo2d
"""

text = """In a world often dominated by negativity, it's important to remember the power of kindness and compassion. Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide. Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.Beyond individual actions, there is also immense power in collective efforts to create positive change. When communities come together to support one another, incredible things can happen. From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.It's also important to recognize the strength that lies within each and every one of us. We all have the ability to make a positive impact, no matter how small our actions may seem. By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.So let's embrace the power of kindness, and strive to make the world a better place one small act at a time. Together, we can create a brighter, more compassionate future for all."""

len(text)

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

nlp = spacy.load('en_core_web_sm')

doc = nlp(text)

# preporcessing text

# tokern
tokens = [token.text.lower() for token in doc
         if not token.is_stop and
         not token.is_punct and
         token.text != '\n']

#calaculate frequency
from collections import Counter

word_freq = Counter(tokens)

# max freq
max_freq = max(word_freq.values())
max_freq

# normialse others
for word in word_freq.keys():
    word_freq[word] = word_freq[word]/max_freq

#sentence tokenization
sent_token = [sent.text for sent in doc.sents]

sent_score = {}
for sent in sent_token:
    for word in sent.split():
        if word.lower() in word_freq.keys():
            if sent not in sent_score.keys():
                sent_score[sent] = word_freq[word]
            else:
                sent_score[sent] += word_freq[word]
        print(word)

sent_score

import pandas as pd

pd.DataFrame(list(sent_score.items()), columns=['Sentence', 'Score'])

from heapq import nlargest

num_sent = 3
n = nlargest(num_sent, sent_score, key=sent_score.get)
" ".join(n)

from transformers import pipeline

summarizer=pipeline("summarization",model='t5-base',tokenizer='t5-base',framework='pt')

text = " Trump is giving his typically rally speech- with the exception of the extended remarks that Prime Minister Takaichi made in Japanese.But once Trump resumed speaking, it was standard fare. He once again claimed that he actually won the 2020 presidential election (which he lost to Joe Biden) and boasted about how much better the nation is doing now.The biggest cheers, not surprisingly, came when Trump said he supported a pay raise for the US military - although he added that Democrats would have to approve it, too."

summary = summarizer(text,max_length=100,min_length=10,do_sample=False)

print(summary[0]['summary_text'])